# Problem 1 DDPM on MNIST (40 points)
In this problem set, you will implement a Denoising Diffusion Probabilistic Model (DDPM) for the MNIST dataset. The DDPM is a generative model that progressively corrupts a data sample through a series of noise additions (forward process) and then learns to reverse this process to generate new samples (reverse process). This assignment will guide you through implementing key components of DDPM and training the model on MNIST.
The forward process in DDPM progressively adds Gaussian noise to an image over T timesteps. 1
The noise schedule is controlled by a series of variance values βt, which determine how much noise is added at each timestep.
The key quantities for the forward process contain: βt: the variance schedule for each timestep, and α ̄t: the cumulative product of αt up to timestep t.
The mathematical formulation for these terms is as follows:
βt =βmin+t·βmax−βmin, t=0,1,2,...,T−1 T−1
where βmin and βmax are hyperparameters controlling the noise at the first and final timesteps. We define αt and α ̄t as:
αt =1−βt t
α ̄ t = Y α i i=1

**Part (a).** Implement the ddpm schedules() function.
The forward process of DDPM gradually adds noise to the data at each timestep t. Given a
data sample x0 and a timestep t, the noisy version xt can be computed as: 
√√
xt= α ̄tx0+ 1−α ̄tε, ε∼N(0,I)

Here, ε is Gaussian noise sampled at each timestep. You will need to use the precomputed
values of α ̄t from the ddpm schedules() function.
(b) Implement forward() to return xt given x0 and timestep t.
Given xT ∼ N(0,I) (a pure Gaussian noise), the reverse process can be written as:

11−αt 
xt−1=√α xt−√1−α ̄εθ(xt,t) +σtz, z∼N(0,I)
tt
where σt is related to the variance at each step.

**(c):** Implement sample() to generate new samples by iteratively applying the reverse
process starting from xT .


**(d):** Train the DDPM model on the MNIST dataset using the code provided


# Problem 2 Flow Matching on MNIST 

In this problem, you will implement Conditional Flow Matching (CFM) for the MNIST dataset. CFM is a generative model that learns a conditional flow field. This assignment will guide you through implementing key components of CFM and training the model on MNIST.
1
In CFM, the model learns a time-dependent vector field that progressively transforms a noise sample into a data sample through a continuous flow. The conditional probability paths for this process are defined by a mean μt(x) and standard deviation σt(x) that change linearly over time. We use Optimal Transport Conditional Vector Field (VF) in this problem.
The key quantities for the flow process are:
μt(x) = tx1, σt(x) = 1 − (1 − σmin)t,
where σmin is a hyperparameter controlling the noise level at t = 1.
According to the flow matching approach, this conditional path is generated by the vector
field:
ut(x|x1) = x1 − (1 − σmin)x. 1−(1−σmin)t

 The conditional flow that corresponds to ut(x|x1) is given by: 
 
 ψt(x) = (1 − (1 − σmin)t)x + tx1.

In this case, the CFM loss is defined as:
LCFM(θ) = Et,q(x1),p(x0) ∥vt(ψt(x0)) − (x1 − (1 − σmin)x0)∥2 .

We use σmin = 0 in this assignment.
(a) Implement the function compute mu t(), which computes the mean μt(x).

(b) Implement the function compute sigma t(), which computes the standard devi- ation σt(x).

(c) Implement the function compute conditional flow(), which calculates the con- ditional vector field ut(x|x1).

(d) Implement the function sample xt(), which draws a sample from the probability path N (μt(x), σt(x)).

(e) Train the CFM model on the MNIST dataset using the code provided
